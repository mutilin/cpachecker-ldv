# This is an auto-generated file, DO NOT EDIT!
# Run ant to generate it.

# Possible log levels in descending order 
# (lower levels include higher ones):
# OFF:      no logs published
# SEVERE:   error messages
# WARNING:  warnings
# INFO:     messages
# FINE:     logs on main application level
# FINER:    logs on central CPA algorithm level
# FINEST:   logs published by specific CPAs
# ALL:      debugging information
# Care must be taken with levels of FINER or lower, as output files may
# become quite large and memory usage might become an issue.

# single levels to be excluded from being logged
log.consoleExclude = []

# log level of console output
log.consoleLevel = Level.INFO

# name of the log file
log.file = "CPALog.txt"

# single levels to be excluded from being logged
log.fileExclude = []

# log level of file output
log.level = Level.OFF

# maximum size of log output strings before they will be truncated
log.truncateSize = 10000

# use colors for log messages on console
log.useColors = true

# all used options are printed
log.usedOptions.export = false

# disable all default output files
# (any explicitly given file will still be written)
output.disable = false

# directory to put all output files in
output.path = "output/"

# base directory for all input & output files
# (except for the configuration file itself)
rootDirectory = "."


# maximum number of condition adjustments (-1 for infinite)
adjustableconditions.adjustmentLimit = -1

# use a BMC like algorithm that checks for satisfiability after the analysis
# has finished, works only with PredicateCPA
analysis.algorithm.BMC = false

# use CBMC as an external tool from CPAchecker
analysis.algorithm.CBMC = false

# use CEGAR algorithm for lazy counter-example guided analysis
# You need to specify a refiner with the cegar.refiner option.
# Currently all refiner require the use of the ARGCPA.
analysis.algorithm.CEGAR = false

# use adjustable conditions algorithm
analysis.algorithm.conditionAdjustment = false

# Use McMillan's Impact algorithm for lazy interpolation
analysis.algorithm.impact = false

# use a predicated analysis which proves if the program satisfies a specified
# property with the help of a PredicateCPA to separate differnt program paths
analysis.algorithm.predicatedAnalysis = false

# use a proof check algorithm to validate a previously generated proof
analysis.algorithm.proofCheck = false

# do analysis and then check if reached set fulfills property specified by
# ConfigurableProgramAnalysisWithPropertyChecker
analysis.algorithm.propertyCheck = false

# use the TestGen Algorithm
analysis.algorithm.testGen = false

# use a second model checking run (e.g., with CBMC or a different CPAchecker
# configuration) to double-check counter-examples
analysis.checkCounterexamples = false

# use counterexample check and the BDDCPA Restriction option
analysis.checkCounterexamplesWithBDDCPARestriction = false

# do analysis and then check analysis result
analysis.checkProof = false

# use assumption collecting algorithm
analysis.collectAssumptions = false

# stop CPAchecker after startup (internal option, not intended for users)
analysis.disable = false

# entry function
analysis.entryFunction = "main"

# create all potential function pointer call edges
analysis.functionPointerCalls = true

# Create edge for skipping a function pointer call if its value is unknown.
analysis.functionPointerEdgesForUnknownPointer = true

# potential targets for call edges created for function pointer calls
analysis.functionPointerTargets = {FunctionSet.USED_IN_CODE, FunctionSet.EQ_PARAM_SIZES}

# run interprocedural analysis
analysis.interprocedural = true

# the machine model, which determines the sizes of types like int
analysis.machineModel = LINUX32
  enum:     [LINUX32, LINUX64]

# A String, denoting the programs to be analyzed
analysis.programNames = no default value

# which reached set implementation to use?
# NORMAL: just a simple set
# LOCATIONMAPPED: a different set per location (faster, states with different
# locations cannot be merged)
# PARTITIONED: partitioning depending on CPAs (e.g Location, Callstack etc.)
analysis.reachedSet = PARTITIONED
  enum:     [NORMAL, LOCATIONMAPPED, PARTITIONED, USAGESTATISTICS]

# restart the analysis using a different configuration after unknown result
analysis.restartAfterUnknown = false

# Save results of local analysis
analysis.saveLocalResults = false

# stop after the first error has been found
analysis.stopAfterError = true

# create summary call statement edges
analysis.summaryEdges = false

# which strategy to adopt for visiting states?
analysis.traversal.order = DFS
  enum:     [DFS, BFS, RAND, RANDOM_PATH]

# handle abstract states with more automaton matches first? (only if
# AutomatonCPA enabled)
analysis.traversal.useAutomatonInformation = false

# handle states with a deeper callstack first?
# This needs the CallstackCPA to have any effect.
analysis.traversal.useCallstack = false

# handle more abstract states (with less information) first? (only for
# ExplicitCPA)
analysis.traversal.useExplicitInformation = false

# Use an implementation of reverse postorder strategy that allows to select a
# secondary strategy that is used if there are two states with the same
# reverse postorder id. The secondary strategy is selected with
# 'analysis.traversal.order'.
analysis.traversal.useReversePostorder = false

# DEPRECATED: This option was renamed to
# analysis.traversal.useReversePostorder and will soon get removed.
analysis.traversal.useTopsort = false

# Do not report unknown if analysis terminated, report true (UNSOUND!).
analysis.unknownAsTrue = false

# stop the analysis with the result unknown if the program does not satisfies
# certain restrictions.
analysis.unknownIfUnrestrictedProgram = false

# add declarations for global variables before entry function
analysis.useGlobalVars = true

# Add a threshold to the automaton, after so many branches on a path the
# automaton will be ignored (0 to disable)
assumptions.automatonBranchingThreshold = 0

# write collected assumptions as automaton to file
assumptions.automatonFile = "AssumptionAutomaton.txt"

# write collected assumptions to file
assumptions.export = true
assumptions.file = "assumptions.txt"

# Size of the BDD cache in relation to the node table size (set to 0 to use
# fixed BDD cache size).
bdd.bddCacheRatio = 0.1

# Size of the BDD cache if cache ratio is not used.
bdd.bddCacheSize = 1000

# Initial size of the BDD node table.
bdd.initBddNodeTableSize = 10000

# Which BDD package should be used?
# - java:  JavaBDD (default, no dependencies, many features)
# - cudd:  CUDD (native library required, reordering not supported)
# - micro: MicroFactory (maximum number of BDD variables is 1024, slow, but
# less memory-comsumption)
# - buddy: Buddy (native library required)
# - cal:   CAL (native library required)
# - jdd:   JDD
bdd.package = "java"
  allowed values: [java, cudd, micro, buddy, cal, jdd]

# Allow reduction of function entries; calculate abstractions alwasy at
# function entries?
blockreducer.allowReduceFunctionEntries = true

# Allow reduction of function exits; calculate abstractions alwasy at
# function exits?
blockreducer.allowReduceFunctionExits = true

# Allow reduction of loop heads; calculate abstractions alwasy at loop heads?
blockreducer.allowReduceLoopHeads = false

# write the reduced cfa to the specified file.
blockreducer.reducedCfaFile = "ReducedCfa.rsf"

# Do at most n summarizations on a node.
blockreducer.reductionThreshold = 100

# If BMC did not find a bug, check whether the bounding did actually remove
# parts of the state space (this is similar to CBMC's unwinding assertions).
bmc.boundingAssertions = true

# Check reachability of target states after analysis (classical BMC). The
# alternative is to check the reachability as soon as the target states are
# discovered, which is done if cpa.predicate.targetStateSatCheck=true.
bmc.checkTargetStates = true

# dump counterexample formula to file
bmc.dumpCounterexampleFormula = "ErrorPath.%d.smt2"

# try using induction to verify programs with loops
bmc.induction = false

# Generate invariants and add them to the induction hypothesis.
bmc.useInvariantsForInduction = false

# File name where to put the path program that is generated as input for
# CBMC. A temporary file is used if this is unspecified. If specified, the
# file name should end with '.i' because otherwise CBMC runs the
# pre-processor on the file.
cbmc.dumpCBMCfile = no default value

# specify the name of the error label
cbmc.options.errorLabel = "ERROR"

# set width of int (16, 32 or 64)
cbmc.options.intWidth = 32

# disable unwinding assertions violation error
cbmc.options.nuaf = false

# specify the limit for unwindings (0 is infinite)
cbmc.options.unwindings = 0

# maximum time limit for CBMC (use milliseconds or specify a unit; 0 for
# infinite)
# maximum time limit for CBMC (0 is infinite)
cbmc.timelimit = 0

# Whether to do refinement immediately after finding an error state, or
# globally after the ARG has been unrolled completely.
cegar.globalRefinement = false

# Which refinement algorithm to use? (give class name, required for CEGAR) If
# the package name starts with 'org.sosy_lab.cpachecker.', this prefix can be
# omitted.
cegar.refiner = no default value

# Which functions should be interpreted as encoding assumptions
cfa.assumeFunctions = {"__VERIFIER_assume"}

# dump a simple call graph
cfa.callgraph.export = true

# file name for call graph as .dot file
cfa.callgraph.file = "functionCalls.dot"

# while this option is activated, before each use of a PointerExpression, or
# a dereferenced field access the expression is checked if it is 0
cfa.checkNullPointers = false

# Whether to have a single target node per function for all invalid null
# pointer dereferences or to have separate nodes for each dereference
cfa.checkNullPointers.singleTargetPerFunction = true

# When a function pointer array element is written with a variable as index,
# create a series of if-else edges with explicit indizes instead.
cfa.expandFunctionPointerArrayAssignments = false

# export CFA as .dot file
cfa.export = true

# export individual CFAs for function as .dot files
cfa.exportPerFunction = true

# export CFA as .dot file
cfa.file = "cfa.dot"

# how often can a function appear in the callstack as a clone of the original
# function?
cfa.functionCalls.recursionDepth = 5

# Also initialize local variables with default values, or leave them
# uninitialized.
cfa.initializeAllVariables = false

# With this option, all declarations in each function will be movedto the
# beginning of each function. Do only use this option if you arenot able to
# handle initializer lists and designated initializers (like they can be used
# for arrays and structs) in your analysis anyway. this option will otherwise
# create c code which is not the same as the original one
cfa.moveDeclarationsToFunctionStart = false

# remove paths from CFA that cannot lead to a specification violation
cfa.removeIrrelevantForSpecification = false

# simplify simple const expressions like 1+2
cfa.simplifyConstExpressions = true

# simplify pointer expressions like s->f to (*s).f with this option the cfa
# is simplified until at maximum one pointer is allowed for left- and
# rightHandSide
cfa.simplifyPointerExpressions = false

# Remove all edges which don't have any effect on the program
cfa.simplyfyCfa = true

# This option causes the control flow automaton to be transformed into the
# automaton of an equivalent program with one single loop and an artificial
# program counter.
cfa.transformIntoSingleLoop = false

# Single loop transformation builds a decision tree based on the program
# counter values. This option causes the last program counter value not to be
# explicitly assumed in the decision tree, so that it is only indirectly
# represented by the assumption of falsehood for all other assumptions in the
# decision tree.
cfa.transformIntoSingleLoop.omitExplicitLastProgramCounterAssumption = true

# This option controls what program counter values are used. Possible values
# are INCREMENTAL and NODE_NUMBER.
cfa.transformIntoSingleLoop.programCounterValueProviderFactory = INCREMENTAL
  enum:     [NODE_NUMBER, INCREMENTAL]

# This option controls the size of the subgraphs referred to by program
# counter values. The larger the subgraphs, the fewer program counter values
# are required. Possible values are  MULTIPLE_PATHS, SINGLE_PATH and
# SINGLE_EDGE, where MULTIPLE_PATHS has the largest subgraphs (and fewest
# program counter values) and SINGLE_EDGE has the smallest subgraphs (and
# most program counter values). The larger the subgraphs, the closer the
# resulting graph will look like the original CFA.
cfa.transformIntoSingleLoop.subgraphGrowthStrategy = no default value
  enum:     [MULTIPLE_PATHS, SINGLE_PATH, SINGLE_EDGE]

# unwind recursive functioncalls (bounded to max call stack size)
cfa.useFunctionCallUnwinding = false

# combine sequences of simple edges into a single edge
cfa.useMultiEdges = false

# Dump domain type statistics to a CSV file.
cfa.variableClassification.domainTypeStatisticsFile = no default value

# Dump variable classification to a file.
cfa.variableClassification.logfile = "VariableClassification.log"

# Print some information about the variable classification.
cfa.variableClassification.printStatsOnStartup = false

# Dump variable type mapping to a file.
cfa.variableClassification.typeMapFile = "VariableTypeMapping.txt"

# Dump the complete configuration to a file.
configuration.dumpFile = "UsedConfiguration.properties"

# which model checker to use for verifying counterexamples as a second check
# Currently CBMC or CPAchecker with a different config can be used.
counterexample.checker = "CBMC"
  allowed values: [CBMC, CPACHECKER]

# configuration file for counterexample checks with CPAchecker
counterexample.checker.config = "config/explicitAnalysis-no-cbmc.properties"

# continue analysis after an counterexample was found that was denied by the
# second check
counterexample.continueAfterInfeasibleError = true

# The files where the BDDCPARestrictionAlgorithm should write the presence
# conditions for the counterexamples to.
counterexample.presenceConditionFile = "ErrorPath.%d.presenceCondition.txt"

# If continueAfterInfeasibleError is true, remove the infeasible
# counterexample before continuing.Setting this to false may prevent a lot of
# similar infeasible counterexamples to get discovered, but is unsound
counterexample.removeInfeasibleErrors = false

# print coverage info to file
coverage.export = true
coverage.file = "coverage.%s.info"

# CPA to use (see doc/Configuration.txt for more documentation on this)
cpa = CompositeCPA.class.getCanonicalName()

# Use this to change the underlying abstract domain in the APRON library
cpa.apron.domain = "OCTAGON"
  allowed values: [BOX, OCTAGON, POLKA, POLKA_STRICT, POLKA_EQ]

# which merge operator to use for ApronCPA?
cpa.apron.merge = "SEP"
  allowed values: [SEP, JOIN]

# export one variable assignment for error path to file, if one is found
cpa.arg.errorPath.assignment = "ErrorPath.%d.assignment.txt"

# export error path to file as an automaton
cpa.arg.errorPath.automaton = "ErrorPath.%d.spc"

# export error path to file, if one is found
cpa.arg.errorPath.core = "ErrorPath.%d.core.txt"
cpa.arg.errorPath.enabled = true
cpa.arg.errorPath.export = true

# translate error path to C program
cpa.arg.errorPath.exportAsSource = true

# export error paths to files immediately after they were found
cpa.arg.errorPath.exportImmediately = false

# export error path to file, if one is found
cpa.arg.errorPath.file = "ErrorPath.%d.txt"

# Filter for irrelevant counterexamples to reduce the number of similar
# counterexamples reported. Only relevant with analysis.stopAfterErrors=false
# and cpa.arg.errorPath.exportImmediately=true. Put the weakest and cheapest
# filter first, e.g., PathEqualityCounterexampleFilter.
cpa.arg.errorPath.filters = ImmutableList.<Class<? extends CounterexampleFilter>>of(
          PathEqualityCounterexampleFilter.class)

# export error path to file, if one is found
cpa.arg.errorPath.graph = "ErrorPath.%d.dot"

# export error path to file as an automaton to a graphml file
cpa.arg.errorPath.graphml = no default value

# export error path to file, if one is found
cpa.arg.errorPath.json = "ErrorPath.%d.json"
cpa.arg.errorPath.source = "ErrorPath.%d.c"

# export final ARG as .dot file
cpa.arg.export = true
cpa.arg.file = "ARG.dot"

# inform ARG CPA if it is run in a predicated analysis because then it
# mustbehave differntly during merge.
cpa.arg.inPredicatedAnalysis = false

# whether to keep covered states in the reached set as addition to keeping
# them in the ARG
cpa.arg.keepCoveredStatesInReached = false

# export simplified ARG that shows all refinements to .dot file
cpa.arg.refinements.file = "ARGRefinements.dot"

# export final ARG as .dot file, showing only loop heads and function
# entries/exits
cpa.arg.simplifiedARG.file = "ARGSimplified.dot"

# Verification witness: Include the considered case of an assume?
cpa.arg.witness.exportAssumeCaseInfo = true

# Verification witness: Include assumptions (C statements)?
cpa.arg.witness.exportAssumptions = true

# Verification witness: Include function calls and function returns?
cpa.arg.witness.exportFunctionCallsAndReturns = true

# Verification witness: Include the (starting) line numbers of the operations
# on the transitions?
cpa.arg.witness.exportLineNumbers = true

# Verification witness: Include the sourcecode of the operations?
cpa.arg.witness.exportSourcecode = true

# Verification witness: Include the token numbers of the operations on the
# transitions?
cpa.arg.witness.exportTokenNumbers = true

# signal the analysis to break in case the given number of error state is
# reached 
cpa.automaton.breakOnTargetState = 1

# Collect information about matched (and traversed) tokens.
cpa.automaton.collectTokenInformation = false

# export automaton to file
cpa.automaton.dotExport = false

# file for saving the automaton in DOT format (%s will be replaced with
# automaton name)
cpa.automaton.dotExportFile = "%s.dot"

# the maximum number of iterations performed after the initial error is
# found, despite the limitgiven as cpa.automaton.breakOnTargetState is not
# yet reached
cpa.automaton.extraIterationsLimit = -1

# file with automaton specification for ObserverAutomatonCPA and
# ControlAutomatonCPA
cpa.automaton.inputFile = no default value

# if enabled, cache queries also consider blocks with non-matching precision
# for reuse.
cpa.bam.aggressiveCaching = true

# export blocked ARG as .dot file
cpa.bam.argFile = "BlockedARG.dot"

# Type of partitioning (FunctionAndLoopPartitioning or
# DelayedFunctionAndLoopPartitioning)
# or any class that implements a PartitioningHeuristic
cpa.bam.blockHeuristic = FunctionPartitioning.class

# export blocks
cpa.bam.exportBlocksPath = "block_cfa.dot"

# if enabled, the reached set cache is analysed for each cache miss to find
# the cause of the miss.
cpa.bam.gatherCacheMissStatistics = false

# export single blocked ARG as .dot files, should contain '%d'
cpa.bam.indexedArgFile = "ARGs/ARG_%d.dot"

# export used parts of blocked ARG as .dot file
cpa.bam.simplifiedArgFile = "BlockedARGSimplified.dot"

# max bitsize for values and vars, initial value
cpa.bdd.bitsize = 64

# use a smaller bitsize for all vars, that have only intEqual values
cpa.bdd.compressIntEqual = true

# Pattern for variablenames that will always be tracked with BDDs.This
# pattern should only be used for known variables, i.e. for boolean vars.
cpa.bdd.forceTrackingPattern = ""

# declare the bits of a var from 0 to N or from N to 0
cpa.bdd.initBitsIncreasing = true

# declare first bit of all vars, then second bit,...
cpa.bdd.initBitwise = true

# declare vars partitionwise
cpa.bdd.initPartitions = Ordered = true

# declare partitions ordered
cpa.bdd.initPartitionsOrdered = true

# whether the precision is initially empty (this should be set to true when
# refinement is used)
cpa.bdd.initiallyEmptyPrecision = false

# Dump tracked variables to a file.
cpa.bdd.logfile = "BDDCPA_tracked_variables.log"

# whether or not to add newly-found variables only to the exact program
# location or to the whole scope of the variable.
cpa.bdd.precision.refinement.useScopedInterpolation = false

# track boolean variables from cfa
cpa.bdd.trackBoolean = true

# track variables, only used in simple calculations (add, sub, gt, lt,
# eq,...) from cfa as bitvectors with (default) 32 bits
cpa.bdd.trackIntAdd = true

# track variables from cfa, that are only compared for equality, they are
# tracked as (small) bitvectors
cpa.bdd.trackIntEqual = true

# depth of recursion bound
cpa.callstack.depth = 0

# Skip recursion if it happens only by going via a function pointer (this is
# unsound). Imprecise function pointer tracking often lead to false
# recursions.
cpa.callstack.skipFunctionPointerRecursion = false

# Skip recursion. Treat function call as a statement (the same as for
# functions without bodies)
# Skip recursion (this is unsound). Treat function call as a statement (the
# same as for functions without bodies)
cpa.callstack.skipRecursion = false

# Skip recursion if it happens only by going via a void function (this is
# unsound).
cpa.callstack.skipVoidRecursion = false

# firing relation to be used in the precision adjustment operator
cpa.chc.firingRelation = "Always"
  allowed values: [Always, Maxcoeff, Sumcoeff, Homeocoeff]

# generalization operator to be used in the precision adjustment operator
cpa.chc.generalizationOperator = "Widen"
  allowed values: [Top, Widen, WidenMax, WidenSum]

# merge operator to be used
cpa.chc.merge = "SEP"
  allowed values: [SEP, JOIN]

# inform Composite CPA if it is run in a predicated analysis because then it
# mustbehave differntly during merge.
cpa.composite.inPredicatedAnalysis = false

# which composite merge operator to use (plain or agree)
# Both delegate to the component cpas, but agree only allows merging if all
# cpas agree on this. This is probably what you want.
cpa.composite.merge = "AGREE"
  allowed values: [PLAIN, AGREE]

# which precision adjustment strategy to use (COMPOSITE or COMPONENT)
# While the COMPOSITE strategy keeps the domain knowledge seperated, and only
# delegates to each component's precision adjustment operator individually,
# the COMPONENT strategy operates with knowledge about all components.
cpa.composite.precAdjust = "COMPOSITE"
  allowed values: [COMPOSITE, COMPONENT]

# Limit for Java heap memory used by CPAchecker (in MiB; -1 for infinite)
cpa.conditions.global.memory.heap = -1

# Limit for process memory used by CPAchecker (in MiB; -1 for infinite)
cpa.conditions.global.memory.process = -1

# Limit for size of reached set (-1 for infinite)
cpa.conditions.global.reached.size = -1

# Limit for cpu time used by CPAchecker (use milliseconds or specify a unit;
# -1 for infinite)
cpa.conditions.global.time.cpu = -1

# Hard limit for cpu time used by CPAchecker (use milliseconds or specify a
# unit; -1 for infinite)
# When using adjustable conditions, analysis will end after this threshold
cpa.conditions.global.time.cpu.hardlimit = -1

# Limit for wall time used by CPAchecker (use milliseconds or specify a unit;
# -1 for infinite)
cpa.conditions.global.time.wall = -1

# Hard limit for wall time used by CPAchecker (use milliseconds or specify a
# unit; -1 for infinite)
# When using adjustable conditions, analysis will end after this threshold
cpa.conditions.global.time.wall.hardlimit = -1

# This option sets the hard threshold for assignments (-1 for infinite). A
# variable reaching this assignment threshold is not tracked anymore, even if
# it is contained in the precision.
cpa.conditions.path.assignments.hardThreshold = -1

# This option sets the soft threshold for assignments (-1 for infinite). The
# semantics are that variables are tracked up to this threshold, even if not
# being contained in the precison yet, and are removed once there are more
# assignments for a variable then defined by this threshold. Once the
# variable is found to be relevant, e.g., through refinement and
# interpolation, the variable is tracked again, until reaching the  hard
# threshold.
cpa.conditions.path.assignments.softThreshold = -1

# maximum number of assume edges length (-1 for infinite)
cpa.conditions.path.assumeedges.limit = -1

# The condition
cpa.conditions.path.condition = no default value

# maximum path length (-1 for infinite)
cpa.conditions.path.length.limit = -1

# maximum repetitions of any edge in a path (-1 for infinite)
cpa.conditions.path.repetitions.limit = -1

# which merge operator to use for DefUseCPA
cpa.defuse.merge = "sep"
  allowed values: [sep, join]

# Which strategy to use for forced coverings (empty for none)
cpa.forcedCovering = no default value

# When an invalid function pointer is called, do not assume all functions as
# possible targets and instead call no function.
cpa.functionpointer.ignoreInvalidFunctionPointerCalls = false

# When an unknown function pointer is called, do not assume all functions as
# possible targets and instead call no function (this is unsound).
cpa.functionpointer.ignoreUnknownFunctionPointerCalls = false

# whether function pointers with invalid targets (e.g., 0) should be tracked
# in order to find calls to such pointers
cpa.functionpointer.trackInvalidFunctionPointers = false

# Highest allowed value of the variable
cpa.interval.allowedHigh = Long.MAX_VALUE

# Lowest allowed value of the variable
cpa.interval.allowedLow = Long.MIN_VALUE

# do not consider number of references to variables during merge in analysis
cpa.interval.ignoreReferenceCounts = false

# which type of merge operator to use for IntervalAnalysisCPA
cpa.interval.merge = "SEP"
  allowed values: [SEP, JOIN]

# decides whether one (false) or two (true) successors should be created when
# an inequality-check is encountered
cpa.interval.splitIntervals = false

# at most that many intervals will be tracked per variable, -1 if number not
# restricted
cpa.interval.threshold = -1

# Variable name of the variable for which should be checked that its value is
# globally in a certain bound 
cpa.interval.varName = ""

# determine variables relevant to the decision whether or not a target path
# assume edge is taken and limit the analyis to those variables.
cpa.invariants.analyzeRelevantVariablesOnly = true

# determine target locations in advance and analyse paths to the target
# locations only.
cpa.invariants.analyzeTargetPathsOnly = true

# controls the condition adjustment logic: STATIC means that condition
# adjustment is a no-op, INTERESTING_VARIABLES increases the interesting
# variable limit, MAXIMUM_FORMULA_DEPTH increases the maximum formula depth,
# ABSTRACTION_STRATEGY tries to choose a more precise abstraction strategy
# and COMPOUND combines the other strategies (minus STATIC).
cpa.invariants.conditionAdjusterFactory = COMPOUND
  enum:     [STATIC, INTERESTING_VARIABLES, MAXIMUM_FORMULA_DEPTH,
             ABSTRACTION_STRATEGY, COMPOUND]

# controls whether to use abstract evaluation always, never, or only on
# already previously visited edges.
cpa.invariants.edgeBasedAbstractionStrategyFactory = VISITED_EDGES
  enum:     [ALWAYS, VISITED_EDGES, NEVER]

# the maximum number of variables to consider as interesting. -1 one disables
# the limit, but this is not recommended. 0 means that guessing interesting
# variables is disabled.
cpa.invariants.interestingVariableLimit = 2

# the maximum tree depth of a formula recorded in the environment.
cpa.invariants.maximumFormulaDepth = 4

# which merge operator to use for InvariantCPA
cpa.invariants.merge = "PRECISIONDEPENDENT"
  allowed values: [JOIN, SEP, PRECISIONDEPENDENT]

# functions, which allocate new free memory
cpa.local.allocatefunctions = no default value

# which merge operator to use for LocalCPA
cpa.local.merge = "JOIN"
  allowed values: [SEP, JOIN]

# which stop operator to use for LocalCPA
cpa.local.stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

#  annotated functions, which are known to works right
cpa.lockStatistics.annotate = no default value

# contains all lock names
cpa.lockStatistics.lockinfo = no default value

# function to reset state
cpa.lockStatistics.lockreset = no default value

# this option controls how the maxLoopIterations condition is adjusted when a
# condition adjustment is invoked.
cpa.loopstack.maxLoopIterationAdjusterFactory = STATIC
  enum:     [STATIC, INCREMENT, DOUBLE]

# threshold for unrolling loops of the program (0 is infinite)
# works only if assumption storage CPA is enabled, because otherwise it would
# be unsound
cpa.loopstack.maxLoopIterations = 0

# threshold for adjusting the threshold for unrolling loops of the program (0
# is infinite).
# only relevant in combination with a non-static maximum loop iteration
# adjuster.
cpa.loopstack.maxLoopIterationsUpperBound = 0

# time limit for a single post computation (use milliseconds or specify a
# unit; 0 for infinite)
cpa.monitor.limit = 0

# time limit for all computations on a path in milliseconds (use milliseconds
# or specify a unit; 0 for infinite)
cpa.monitor.pathcomputationlimit = 0

# with this option the evaluation of float variables can be toggled.
# Additionally the used number representation in the library will be changed
# between floats and ints.
cpa.octagon.handleFloats = false

# whether or not to use use-definition information from the error pathsto
# optimize the interpolation process
cpa.octagon.interpolation.applyUseDefInformation = true

# whether or not to ignore the semantics of loop-leaving-assume-edges during
# interpolation - this avoids to have loop-counters in the interpolant
cpa.octagon.interpolation.ignoreLoopsExitAssumes = true

# which merge operator to use for OctagonCPA?
cpa.octagon.merge = "SEP"
  allowed values: [SEP, JOIN]

# with this option enabledmergeJoin is only used on edges within the same
# block, i.e. each iterationof a loop is a different block, thus the
# precision of the analysis increases
cpa.octagon.mergeop.onlyJoinEdgesInSameBlock = false

# of which type should the merge be? normal, for usual join, widening for a
# widening instead of a join
cpa.octagon.mergeop.type = "NORMAL"
  allowed values: [NORMAL, WIDENING]

# turn the refiner on or off, default is off
cpa.octagon.refiner = false

# whether or not to avoid restarting at assume edges after a refinement
cpa.octagon.refiner.avoidAssumes = false

# whether or not to check for repeated refinements, to then reset the
# refinement root
cpa.octagon.refiner.checkForRepeatedRefinements = true

# whether or not to do lazy-abstraction
cpa.octagon.refiner.doLazyAbstraction = true

# Timelimit (in seconds) for the backup feasibility check with the octagon
# analysis. Zero means there is no timelimit.
cpa.octagon.refiner.timeForOctagonFeasibilityCheck = 0

# which merge operator to use for InvariantCPA
cpa.pointer2.merge = "JOIN"
  allowed values: [JOIN, SEP]

# which merge operator to use for PointerACPA
cpa.pointerA.merge = "JOIN"
  allowed values: [SEP, JOIN]

# which stop operator to use for PointerACPA
cpa.pointerA.stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

# use caching of region to formula conversions
# use caching of abstractions
cpa.predicate.abs.useCache = true

# DEPRECATED: whether to use Boolean (false) or Cartesian (true) abstraction
cpa.predicate.abstraction.cartesian = false

# whether to use Boolean or Cartesian abstraction or both
cpa.predicate.abstraction.computation = BOOLEAN
  enum:     [CARTESIAN, BOOLEAN, COMBINED]

# dump the abstraction formulas if they took to long
cpa.predicate.abstraction.dumpHardQueries = false

# Identify those predicates where the result is trivially known before
# abstraction computation and omit them.
cpa.predicate.abstraction.identifyTrivialPredicates = false

# get an initial map of predicates from a list of files (see source
# doc/examples/predmap.txt for an example)
cpa.predicate.abstraction.initialPredicates = []

# Apply location-specific predicates to all locations in their function
cpa.predicate.abstraction.initialPredicates.applyFunctionWide = false

# Apply location- and function-specific predicates globally (to all locations
# in the program)
cpa.predicate.abstraction.initialPredicates.applyGlobally = false

# An initial set of comptued abstractions that might be reusable
cpa.predicate.abstraction.reuseAbstractionsFrom = no default value

# What to use for storing abstractions
cpa.predicate.abstraction.type = "BDD"
  allowed values: [BDD, FORMULA]

# file that consists of one abstraction formula for each abstraction state
cpa.predicate.abstractions.file = "abstractions.txt"

# whether to use auxiliary predidates for reduction
cpa.predicate.bam.auxiliaryPredicateComputer = true

# force abstractions immediately after threshold is reached (no effect if
# threshold = 0)
cpa.predicate.blk.alwaysAfterThreshold = true

# abstraction always and only on explicitly computed abstraction nodes.
cpa.predicate.blk.alwaysAndOnlyAtExplicitNodes = false

# force abstractions at each function calls/returns, regardless of threshold
cpa.predicate.blk.alwaysAtFunctions = true

# force abstractions at each join node, regardless of threshold
cpa.predicate.blk.alwaysAtJoin = false

# force abstractions at loop heads, regardless of threshold
cpa.predicate.blk.alwaysAtLoops = true

# abstractions at function calls/returns if threshold has been reached (no
# effect if threshold = 0)
cpa.predicate.blk.functions = false

# abstractions at CFA nodes with more than one incoming edge if threshold has
# been reached (no effect if threshold = 0)
cpa.predicate.blk.join = false

# abstractions at loop heads if threshold has been reached (no effect if
# threshold = 0)
cpa.predicate.blk.loops = false

# maximum blocksize before abstraction is forced
# (non-negative number, special values: 0 = don't check threshold, 1 = SBE)
cpa.predicate.blk.threshold = 0

# use caching of path formulas
cpa.predicate.blk.useCache = true

# always check satisfiability at end of block, even if precision is empty
cpa.predicate.checkBlockFeasibility = false

# Check whether all pointer dereferences are valid.
cpa.predicate.checkValidDeref = false

# Check whether all calls to free() are valid.
cpa.predicate.checkValidFree = false

# The default size in bytes for memory allocations when the value cannot be
# determined.
cpa.predicate.defaultAllocationSize = 4

# The default length for arrays when the real length cannot be determined.
cpa.predicate.defaultArrayLength = 20

# Use deferred allocation heuristic that tracks void * variables until the
# actual type of the allocation is figured out.
cpa.predicate.deferUntypedAllocations = true

# Enable the possibility to precompute explicit abstraction locations.
cpa.predicate.enableBlockreducer = false

# Theory to use as backend for bitvectors. If different from BITVECTOR, the
# specified theory is used to approximate bitvectors. This can be used for
# solvers that do not support bitvectors, or for increased performance.
cpa.predicate.encodeBitvectorAs = RATIONAL
  enum:     [INTEGER, RATIONAL, BITVECTOR]

# Name of an external function that will be interpreted as if the function
# call would be replaced by an externally defined expression over the program
# variables. This will only work when all variables referenced by the dimacs
# file are global and declared before this function is called.
cpa.predicate.externModelFunctionName = "__VERIFIER_externModelSatisfied"

# where to dump interpolation and abstraction problems (format string)
cpa.predicate.formulaDumpFilePattern = "%s%04d-%s%03d.smt2"

# Handle field access via extract and concat instead of new variables.
cpa.predicate.handleFieldAccess = false

# If disabled, all implicitly initialized fields and elements are treated as
# non-dets
cpa.predicate.handleImplicitInitialization = true

# Handle aliasing of pointers. This adds disjunctions to the formulas, so be
# careful when using cartesian abstraction.
cpa.predicate.handlePointerAliasing = true

# When a string literal initializer is encountered, initialize the contents
# of the char array with the contents of the string literal instead of just
# assigning a fresh non-det address to it
cpa.predicate.handleStringLiteralInitializers = false

# Allows to ignore Concat and Extract Calls when Bitvector theory was
# replaced with Integer or Rational.
cpa.predicate.ignoreExtractConcat = true

# Ignore variables that are not relevant for reachability properties.
cpa.predicate.ignoreIrrelevantVariables = true

# Which solver to use specifically for interpolation (default is to use the
# main one).
cpa.predicate.interpolationSolver = no default value
  enum:     [MATHSAT5, SMTINTERPOL, Z3]

# export final loop invariants
cpa.predicate.invariants.export = true

# export invariants as precision file?
cpa.predicate.invariants.exportAsPrecision = true

# file for exporting final loop invariants
cpa.predicate.invariants.file = "invariants.txt"

# file for precision that consists of invariants.
cpa.predicate.invariants.precisionFile = "invariantPrecs.txt"

# List of further options which will be passed to Mathsat in addition to the
# default options. Format is 'key1=value1,key2=value2'
cpa.predicate.mathsat5.furtherOptions = ""

# Export solver queries in Smtlib format into a file (for Mathsat5).
cpa.predicate.mathsat5.logAllQueries = false
cpa.predicate.mathsat5.logfile = "mathsat5.%03d.smt2"

# Max. number of edge of the abstraction tree to prescan for reuse
cpa.predicate.maxAbstractionReusePrescan = 1

# The maximum length for arrays (elements beyond this will be ignored).
cpa.predicate.maxArrayLength = 20

# Maximum size of allocations for which all structure fields are regarded
# always essential, regardless of whether they were ever really used in code.
cpa.predicate.maxPreFilledAllocationSize = 0

# Set of functions that non-deterministically provide new memory on the heap,
# i.e. they can return either a valid pointer or zero.
cpa.predicate.memoryAllocationFunctions = {
      "malloc", "__kmalloc", "kmalloc"
      }

# Memory allocation functions of which all parameters but the first should be
# ignored.
cpa.predicate.memoryAllocationFunctionsWithSuperfluousParameters = {
      "__kmalloc", "kmalloc", "kzalloc"}

# Set of functions that non-deterministically provide new zeroed memory on
# the heap, i.e. they can return either a valid pointer or zero.
cpa.predicate.memoryAllocationFunctionsWithZeroing = {"kzalloc", "calloc"}

# Setting this to true makes memoryAllocationFunctions always return a valid
# pointer.
cpa.predicate.memoryAllocationsAlwaysSucceed = false

# Function that is used to free allocated memory.
cpa.predicate.memoryFreeFunctionName = "free"

# which merge operator to use for predicate cpa (usually ABE should be used)
cpa.predicate.merge = "ABE"
  allowed values: [SEP, ABE]

# Set of functions that should be considered as giving a non-deterministic
# return value. If you specify this option, the default values are not added
# automatically to the list, so you need to specify them explicitly if you
# need them. Mentioning a function in this list has only an effect, if it is
# an 'external function', i.e., no source is given in the code for this
# function.
cpa.predicate.nondetFunctions = {
      "sscanf",
      "random"}

# Regexp pattern for functions that should be considered as giving a
# non-deterministic return value (c.f. cpa.predicate.nondedFunctions)
cpa.predicate.nondetFunctionsRegexp = "^(__VERIFIER_)?nondet_[a-zA-Z0-9_]*"

# Where to apply the found predicates to?
cpa.predicate.precision.sharing = LOCATION
  enum:     [GLOBAL, FUNCTION, LOCATION, LOCATION_INSTANCE]

# export final predicate map
cpa.predicate.predmap.export = true

# file for exporting final predicate map
cpa.predicate.predmap.file = "predmap.txt"

# Format for exporting predicates from precisions.
cpa.predicate.predmap.predicateFormat = SMTLIB2
  enum:     [PLAIN, SMTLIB2]

# If an abstraction is computed during refinement, use only the interpolant
# as input, not the concrete block.
cpa.predicate.refinement.abstractInterpolantOnly = false

# refinement will try to build 'well-scoped' predicates, by cutting spurious
# traces as explained in Section 5.2 of the paper 'Abstractions From Proofs'
# (this does not work with function inlining).
# THIS FEATURE IS CURRENTLY NOT AVAILABLE. 
cpa.predicate.refinement.addWellScopedPredicates = false

# use only the atoms from the interpolants as predicates, and not the whole
# interpolant
cpa.predicate.refinement.atomicPredicates = true

# Direction for doing counterexample analysis: from start of trace, from end
# of trace, or alternatingly from start and end of the trace towards the
# middle
cpa.predicate.refinement.cexTraceCheckDirection = FORWARDS
  enum:     [FORWARDS, BACKWARDS, ZIGZAG]

# Actually compute an abstraction, otherwise just convert the interpolants to
# BDDs as they are.
cpa.predicate.refinement.doAbstractionComputation = false

# where to dump the counterexample formula in case the error location is
# reached
cpa.predicate.refinement.dumpCounterexampleFile = "ErrorPath.%d.smt2"

# dump all interpolation problems
cpa.predicate.refinement.dumpInterpolationProblems = false

# After each refinement, dump the newly found predicates.
cpa.predicate.refinement.dumpPredicates = false

# File name for the predicates dumped after refinements.
cpa.predicate.refinement.dumpPredicatesFile = "refinement%04d-predicates.prec"

# apply deletion-filter to the abstract counterexample, to get a minimal set
# of blocks, before applying interpolation-based refinement
cpa.predicate.refinement.getUsefulBlocks = false

# use incremental search in counterexample analysis, to find the minimal
# infeasible prefix
cpa.predicate.refinement.incrementalCexTraceCheck = false

# During refinement, keep predicates from all removed parts of the ARG.
# Otherwise, only predicates from the error path are kept.
cpa.predicate.refinement.keepAllPredicates = false

# skip refinement if input formula is larger than this amount of bytes
# (ignored if 0)
cpa.predicate.refinement.maxRefinementSize = 0

# use heuristic to extract predicates from the CFA statically on first
# refinement
cpa.predicate.refinement.performInitialStaticRefinement = false

# Do a complete restart (clearing the reached set) after N refinements. 0 to
# disable, 1 for always.
cpa.predicate.refinement.restartAfterRefinements = 0

# Use a single SMT solver environment for several interpolation queries
cpa.predicate.refinement.reuseInterpolationEnvironment = false

# During refinement, add all new predicates to the precisions of all abstract
# states in the reached set.
cpa.predicate.refinement.sharePredicates = false

# slice block formulas, experimental feature!
cpa.predicate.refinement.sliceBlockFormulas = false

# split each arithmetic equality into two inequalities when extracting
# predicates from interpolants
cpa.predicate.refinement.splitItpAtoms = false

# time limit for refinement (use milliseconds or specify a unit; 0 for
# infinite)
cpa.predicate.refinement.timelimit = 0

# Use BDDs to simplify interpolants (removing irrelevant predicates)
cpa.predicate.refinement.useBddInterpolantSimplification = false

# verify if the interpolants fulfill the interpolant properties
cpa.predicate.refinement.verifyInterpolants = false

# Enable the option to allow detecting the allocation type by type of the LHS
# of the assignment, e.g. char *arr = malloc(size) is detected as char[size]
cpa.predicate.revealAllocationTypeFromLhs = true

# maximum blocksize before a satisfiability check is done
# (non-negative number, 0 means never, if positive should be smaller than
# blocksize)
cpa.predicate.satCheck = 0

# Double check generated results like interpolants and models whether they
# are correct
cpa.predicate.smtinterpol.checkResults = false

# List of further options which will be set to true for SMTInterpol in
# addition to the default options. Format is 'option1,option2,option3'
cpa.predicate.smtinterpol.furtherOptions = []

# Export solver queries in Smtlib format into a file.
cpa.predicate.smtinterpol.logAllQueries = false

# Export interpolation queries in Smtlib format into a file.
cpa.predicate.smtinterpol.logInterpolationQueries = false

# Export solver queries in Smtlib format into a file.
cpa.predicate.smtinterpol.logfile = "smtinterpol.%03d.smt2"

# Whether to use MathSAT 5, SmtInterpol or Z3 as SMT solver (Z3 needs the
# FOCI library from http://www.kenmcmil.com/foci2/).
cpa.predicate.solver = MATHSAT5
  enum:     [MATHSAT5, SMTINTERPOL, Z3]

# log some solver actions, this may be slow!
cpa.predicate.solver.useLogger = false

# Export solver queries in Smtlib2 format.
cpa.predicate.solver.z3.logger.logfile = "z3smtlog.%d.smt2"

# Export solver queries in Smtlib2 format, there are small differences for
# different solvers, choose target-solver.
cpa.predicate.solver.z3.logger.target = Z3
  allowed values: [Z3, MATHSAT5]

# simplify formulas when they are asserted in a solver.
cpa.predicate.solver.z3.simplifyFormulas = false

# The function used to model successful heap object allocation. This is only
# used, when pointer analysis with UFs is enabled.
cpa.predicate.successfulAllocFunctionName = "__VERIFIER_successful_alloc"

# The function used to model successful heap object allocation with zeroing.
# This is only used, when pointer analysis with UFs is enabled.
cpa.predicate.successfulZallocFunctionName = "__VERIFIER_successful_zalloc"

# whether to include the symbolic path formula in the coverage checks or do
# only the fast abstract checks
cpa.predicate.symbolicCoverageCheck = false

# check satisfiability when a target state has been found (should be true)
cpa.predicate.targetStateSatCheck = true

# try to add some useful static-learning-like axioms for bitwise operations
# (which are encoded as UFs): essentially, we simply collect all the numbers
# used in bitwise operations, and add axioms like (0 & n = 0)
cpa.predicate.useBitwiseAxioms = false

# Generate invariants and strengthen the formulas during abstraction with
# them.
cpa.predicate.useInvariantsForAbstraction = false

# add special information to formulas about non-deterministic functions
cpa.predicate.useNondetFlags = false

# Qualified name for class which checks that the computed abstraction adheres
# to the desired property.
cpa.propertychecker.className = "org.sosy_lab.cpachecker.pcc.propertychecker.DefaultPropertyChecker"

# List of parameters for constructor of propertychecker.className. Parameter
# values are specified in the order the parameters are defined in the
# respective constructor. Every parameter value is finished with ",". The
# empty string represents an empty parameter list.
cpa.propertychecker.parameters = ""

# which merge operator to use for ReachingDefCPA
cpa.reachdef.merge = "JOIN"
  allowed values: [SEP, JOIN, IGNORECALLSTACK]

# which stop operator to use for ReachingDefCPA
cpa.reachdef.stop = "SEP"
  allowed values: [SEP, JOIN, IGNORECALLSTACK]

# path to a file with abstraction rules
cpa.seplogic.partingstar.abstractionfile = no default value

# path to a file with logic rules
cpa.seplogic.partingstar.logicsfile = no default value

# path to partingstar command
cpa.seplogic.partingstar.pspath = no default value

# Abstract value describing all values which are allowed for the variable
# which is checked
cpa.sign.allowedAbstractValue = ALL
  enum:     [EMPTY, PLUS, MINUS, ZERO, PLUSMINUS, PLUS0, MINUS0, ALL]

# which merge operator to use for SignCPA
cpa.sign.merge = "JOIN"
  allowed values: [SEP, JOIN]

# which stop operator to use for SignCPA
cpa.sign.stop = "SEP"
  allowed values: [SEP, JOIN]

# Variable name of the variable for which should be checked that its value is
# globally in a certain bound 
cpa.sign.varName = ""

# with this option enabled, a check for unreachable memory occurs whenever a
# function returns, and not only at the end of the main function
cpa.smg.checkForMemLeaksAtEveryFrameDrop = true

# If this Option is enabled, failure of mallocis simulated
cpa.smg.enableMallocFail = true

# Filename format for SMG graph dumps
cpa.smg.exportSMG.file = "smg-%s.dot"

# Describes when SMG graphs should be dumped. One of: {never, leaf,
# interesting, every}
cpa.smg.exportSMGwhen = "never"

# with this option enabled, memory that is not freed before the end of main
# is reported as memleak even if it is reachable from local variables in main
cpa.smg.handleNonFreedMemoryInMainAsMemLeak = false

# Sets how unknown functions are handled. One of: {strict, assume_safe}
cpa.smg.handleUnknownFunctions = "strict"

# Determines if memory errors are target states
cpa.smg.memoryErrors = true

# Sets the level of runtime checking: NONE, HALF, FULL
cpa.smg.runtimeCheck = NONE
  enum:     [FORCED, NONE, HALF, FULL]

# Emit messages when we encounter non-target undefined behavior
cpa.smg.unknownOnUndefined = true

# with this option enabled, a check for unreachable memory occurs whenever a
# function returns, and not only at the end of the main function
cpa.smgfork.checkForMemLeaksAtEveryFrameDrop = true

# If this Option is enabled, failure of mallocis simulated
cpa.smgfork.enableMallocFail = true

# Filename format for SMG graph dumps
cpa.smgfork.exportSMG.file = "smg-%s.dot"

# Describes when SMG graphs should be dumped. One of: {never, leaf,
# interesting, every}
cpa.smgfork.exportSMGwhen = "never"

# with this option enabled, memory that is not freed before the end of main
# is reported as memleak even if it is reachable from local variables in main
cpa.smgfork.handleNonFreedMemoryInMainAsMemLeak = false

# Sets how unknown functions are handled. One of: {strict, assume_safe}
cpa.smgfork.handleUnknownFunctions = "strict"

# Determines if memory errors are target states
cpa.smgfork.memoryErrors = true

# Sets the level of runtime checking: NONE, HALF, FULL
cpa.smgfork.runtimeCheck = NONE
  enum:     [FORCED, NONE, HALF, FULL]

# Emit messages when we encounter non-target undefined behavior
cpa.smgfork.unknownOnUndefined = true

# set this to true when you only want to do a code analysis. If StatisticsCPA
# is combined with other CPAs to do queries use false.
cpa.statistics.analysis = true

# which merge operator to use for StatisticsCPA? Ignored when analysis is set
# to true
cpa.statistics.mergeSep = "sep"
  allowed values: [sep, join]

# count the number of traversed arithmetic operations.
cpa.statistics.metric.arithmeticOperationCount = true

# count the number of traversed variable definitions with array type.
cpa.statistics.metric.arrayVariablesCount = true

# count the number of traversed assume statements.
cpa.statistics.metric.assumeCount = true

# count the number of traversed bitwise operations.
cpa.statistics.metric.bitwiseOperationCount = true

# count the number of traversed edges with more then one outgoing edge.
cpa.statistics.metric.branchCount = true

# count the number of traversed dereference operations.
cpa.statistics.metric.dereferenceCount = true

# count the number of traversed variable definitions with floating type
# (float or double).
cpa.statistics.metric.floatVariablesCount = true

# count the number of traversed function calls.
cpa.statistics.metric.functionCallCount = true

# count the number of traversed function definitions.
cpa.statistics.metric.functionDefCount = true

# count the number of traversed global variable definitions.
cpa.statistics.metric.globalVariablesCount = true

# count the number of traversed gotos.
cpa.statistics.metric.gotoCount = true

# count the number of traversed variable definitions with integer type.
cpa.statistics.metric.integerVariablesCount = true

# count the number of traversed jumps.
cpa.statistics.metric.jumpCount = true

# count the number of traversed local variable definitions.
cpa.statistics.metric.localVariablesCount = true

# count the number of traversed loops.
cpa.statistics.metric.loopCount = true

# count the number of traversed nodes.
cpa.statistics.metric.nodeCount = true

# count the number of traversed variable definitions with pointer type.
cpa.statistics.metric.pointerVariablesCount = true

# count the number of traversed variable definitions with a complex structure
# type.
cpa.statistics.metric.structVariablesCount = true

# set to true if you want to print the statistics in the standard output.
cpa.statistics.printOut = true

# target file to hold the statistics
cpa.statistics.statisticsCPAFile = no default value

# if enabled checks if states are target states (there exist warning for
# uninitilized use of variables
cpa.uninitvars.checkTarget = false

# which merge operator to use for UninitializedVariablesCPA?
cpa.uninitvars.merge = "sep"
  allowed values: [sep, join]

# print warnings during analysis when uninitialized variables are used
cpa.uninitvars.printWarnings = "true"

# which stop operator to use for UninitializedVariablesCPA?
cpa.uninitvars.stop = "sep"
  allowed values: [sep, join]

# functions, which stops analysis
cpa.usagestatistics.abortfunctions = no default value

# functions, which are used to bind variables (like list elements are binded
# to list variable)
cpa.usagestatistics.binderFunctions = no default value

# should we use local analysis?
cpa.usagestatistics.localanalysis = false

# which merge operator to use for LockStatisticsCPA
cpa.usagestatistics.merge = "SEP"
  allowed values: [SEP, JOIN]

# path to write results
cpa.usagestatistics.output = "unsafe_rawdata"

# print all unsafe cases in report
cpa.usagestatistics.printAllUnsafeUsages = false

# functions, which we don't analize
cpa.usagestatistics.skippedfunctions = no default value

# variables, which will not be saved in statistics
cpa.usagestatistics.skippedvariables = no default value

# which stop operator to use for LockStatisticsCPA
cpa.usagestatistics.stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

# Value which is allowed for the variable which is checked
cpa.value.allowedValue = 0

# Process the Automaton ASSUMEs as if they were statements, not as if they
# were assumtions.
cpa.value.automatonAssumesAsStatements = false

# restrict abstractions to assume edges
cpa.value.blk.alwaysAtAssumes = false

# restrict abstractions to function calls/returns
cpa.value.blk.alwaysAtFunctions = false

# restrict abstractions to join points
cpa.value.blk.alwaysAtJoins = false

# restrict abstractions to loop heads
cpa.value.blk.alwaysAtLoops = false

# enables target checking for value-analysis, needed for predicate-analysis
cpa.value.doTargetCheck = false

# enable if will be used in predicated analysis but all variables should be
# tracked, no refinement
cpa.value.inPredicatedAnalysis = false

# if there is an assumption like (x!=0), this option sets unknown
# (uninitialized) variables to 1L, when the true-branch is handled.
cpa.value.initAssumptionVars = false

# get an initial precison from file
cpa.value.initialPrecisionFile = no default value

# whether or not to use use-definition information from the error pathsto
# optimize the interpolation process
cpa.value.interpolation.applyUseDefInformation = true

# whether or not to ignore the semantics of loop-leaving-assume-edges during
# interpolation - this avoids to have loop-counters in the interpolant
cpa.value.interpolation.ignoreLoopsExitAssumes = true

# which merge operator to use for ValueAnalysisCPA
cpa.value.merge = "SEP"
  allowed values: [SEP, JOIN]

# ignore boolean variables. if this option is used, booleans from the cfa
# should tracked with another CPA, i.e. with BDDCPA.
cpa.value.precision.ignoreBoolean = false

# ignore variables, that are only used in simple calculations (add, sub, lt,
# gt, eq). if this option is used, these variables from the cfa should
# tracked with another CPA, i.e. with BDDCPA.
cpa.value.precision.ignoreIntAdd = false

# ignore variables, that are only compared for equality. if this option is
# used, these variables from the cfa should tracked with another CPA, i.e.
# with BDDCPA.
cpa.value.precision.ignoreIntEqual = false

# the threshold which controls whether or not variable valuations ought to be
# abstracted once the specified number of valuations per variable is reached
# in the set of reached states
cpa.value.precision.reachedSetThreshold = -1

# whether to track relevant variables only at the exact program location
# (sharing=location), or within their respective (function-/global-) scope
# (sharing=scoped).
cpa.value.precision.sharing = "scope"
  allowed values: [location, scope]

# target file to hold the exported precision
cpa.value.precisionFile = no default value

# when to export the interpolation tree
# NEVER:   never export the interpolation tree
# FINAL:   export the interpolation tree once after each refinement
# ALWAYD:  export the interpolation tree once after each interpolation, i.e.
# multiple times per refinmenet
cpa.value.refinement.exportInterpolationTree = "NEVER"
  allowed values: [NEVER, FINAL, ALWAYS]

# whether or not to do lazy-abstraction
cpa.value.refinement.restart = TOP
  enum:     [TOP, BOTTOM, COMMON]

# whether to use the top-down interpolation strategy or the bottom-up
# interpolation strategy
cpa.value.refinement.useTopDownInterpolationStrategy = true

# whether or not to avoid restarting at assume edges after a refinement
cpa.value.refiner.avoidAssumes = false

# whether or not to check for repeated refinements, to then reset the
# refinement root
cpa.value.refiner.checkForRepeatedRefinements = true

# whether or not to do lazy-abstraction
cpa.value.refiner.doLazyAbstraction = true

# whether or not to interpolate the shortest infeasible prefix rather than
# the whole error path
cpa.value.refiner.interpolateInfeasiblePrefix = false

# use heuristic to extract a precision from the CFA statically on first
# refinement
cpa.value.refiner.performInitialStaticRefinement = false

# which stop operator to use for ValueAnalysisCPA
cpa.value.stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

# enables generation of symbolic values
cpa.value.symbolicValues = false

# Variable name of the variable for which should be checked that its value is
# globally a certain value 
cpa.value.varName = ""

# blacklist regex for variables that won't be tracked by ValueAnalysisCPA
cpa.value.variableBlacklist = ""

# enable the Forced Covering optimization
impact.useForcedCovering = true

# adjust invariant generation conditions if supported by the analysis
invariantGeneration.adjustConditions = false

# generate invariants in parallel to the normal analysis
invariantGeneration.async = false

# configuration file for invariant generation
invariantGeneration.config = no default value

# Specify the class code path to search for java class or interface
# definitions
java.classpath = ""

# use the following encoding for java files
java.encoding = StandardCharsets.UTF_8

# export TypeHierarchy as .dot file
java.exportTypeHierarchy = true

# Specify the source code path to search for java class or interface
# definitions
java.sourcepath = ""

# export TypeHierarchy as .dot file
java.typeHierarchyFile = "typeHierarchy.dot"

# Specifies the java version of source code accepted
java.version = JavaCore.VERSION_1_7

# C or Java?
language = C
  enum:     [C, JAVA]

# Limit for cpu time used by CPAchecker (use seconds or specify a unit; -1
# for infinite)
limits.time.cpu = -1

# Limit for wall time used by CPAchecker (use seconds or specify a unit; -1
# for infinite)
limits.time.wall = -1

# Write the tokenized version of the input program to this file.
locmapper.dumpTokenizedProgramToFile = no default value

# Whether to check for memory safety properties (this can be specified by
# passing an appropriate .prp file to the -spec parameter).
memorysafety.check = false

# When checking for memory safety properties, use this configuration file
# instead of the current one.
memorysafety.config = no default value

# C dialect for parser
parser.dialect = GNUC
  enum:     [C99, GNUC]

# The command line for calling the preprocessor. May contain binary name and
# arguments, but won't be expanded by a shell. The source file name will be
# appended to this string. The preprocessor needs to print the output to
# stdout.
parser.preprocessor = "cpp"

# For C files, read #line preprocessor directives and use their information
# for outputting line numbers. (Always enabled when pre-processing is used.)
parser.readLineDirectives = false

# Preprocess the given C files before parsing: Put every single token onto a
# new line. Then the line number corresponds to the token number.
parser.transformTokensToLines = false

# For C files, run the preprocessor on them before parsing. Note that all
# file numbers printed by CPAchecker will refer to the pre-processed file,
# not the original input file.
parser.usePreprocessor = false

# Enable if used property checker implements satisfiesProperty(AbstractState)
# and checked property is violated for a set iff an element in this set
# exists for which violates the property
pcc.checkPropertyPerElement = false

# The number of cores used exclusively for proof reading. Must be less than
# pcc.useCores and may not be negative. Value 0 means that the cores used for
# reading and checking are shared
pcc.interleaved.useReadCores = 0

# enables parallel checking of partial certificate
pcc.parallel.io.enableParallelCheck = false

# Selects the strategy used for partial certificate construction
pcc.partial.certificateType = HEURISTIC
  enum:     [ALL, HEURISTIC, ARG, MONOTONESTOPARG]

# If enabled, distributes checking of partial elements depending on actual
# checking costs, else uses the number of elements
pcc.partial.enableLoadDistribution = false

# Enables proper PCC but may not work correctly for heuristics. Stops adding
# newly computed elements to reached set if size saved in proof is reached.
# If another element must be added, stops certificate checking and returns
# false.
pcc.partial.stopAddingAtReachedSetSize = false

# Specifies the maximum size of the partition. This size is used to compute
# the number of partitions if a proof (reached set) should be written.
# Default value 0 means always a single partition.
pcc.partitioning.maxNumElemsPerPartition = 0

# Heuristic for computing partitioning of proof (partial reached set).
pcc.partitioning.partitioningStrategy = RANDOM
  enum:     [RANDOM, OPTIMAL]

# If enabled uses the number of nodes saved in certificate size to compute
# partition number otherwise the size of certificate
pcc.partitioning.useGraphSizeToComputePartitionNumber = false

# file in which proof representation needed for proof checking is stored
pcc.proofFile = "arg.obj"

# Generate and dump a proof
pcc.proofgen.doPCC = false

# Qualified name for class which implements certification strategy, hence
# proof writing, to be used.
# Qualified name for class which implements proof checking strategy to be
# used.
pcc.strategy = "org.sosy_lab.cpachecker.pcc.strategy.ARGProofCheckerStrategy"

# number of cpus/cores which should be used in parallel for proof checking
pcc.useCores = 1

# print reached set to text file
reachedSet.export = true
reachedSet.file = "reached.txt"

# if it's true, we always run other algorithm, even previous is sound and
# finished correctly
restartAlgorithm.alwaysrun = false

# list of files with configurations to use
# List of files with configurations to use. A filename can be suffixed with
# :if-interrupted, :if-failed, and :if-terminated which means that this
# configuration will only be used if the previous configuration ended with a
# matching condition.
restartAlgorithm.configFiles = no default value

spec.automatonDumpFile = no default value

# Consider the negative semantics of tokens provided with path automatons.
spec.considerNegativeSemanticsAttribute = true

# Match the line numbers within the origin (mapping done by preprocessor line
# markers).
spec.matchOriginLine = false

# Match the source code provided with the witness.
spec.matchSourcecodeData = false

spec.transitionToStopForNegatedTokensetMatch = true

# comma-separated list of files with specifications that should be checked
# (see config/specification/ for examples)
specification = no default value

# Add all assumtions from the control flow automaton to the precision.
staticRefiner.addAllControlFlowAssumes = false

# Add all assumtions along a error trace to the precision.
staticRefiner.addAllErrorTraceAssumes = false
staticRefiner.addAssumesByBoundedBackscan = true

# Apply mined predicates on the corresponding scope. false = add them to the
# global precision.
staticRefiner.applyScoped = true

# Dump CFA assume edges as SMTLIB2 formulas to a file.
staticRefiner.assumePredicatesFile = no default value

# collect at most this number of assumes along a path, backwards from each
# target (= error) location
staticRefiner.maxBackscanPathAssumes = 1

# write some statistics to disk
statistics.export = true
statistics.file = "Statistics.txt"

# track memory usage of JVM during runtime
statistics.memory = true

# print statistics to console
statistics.print = false

# The path selector for TestGenAlgorithm
testgen.pathSelector = CUTE_PATH_SELECTOR
  enum:     [LOCATION_AND_VALUE_STATE_TRACKING, CFA_TRACKING, CUTE_PATH_SELECTOR,
             CUTE_LIKE]

# Set this to true to get the automaton files for exploring new Paths. You
# also get the ARG as dot file and the local reached set for every algoritm
# iteration in subdirs under output.
testgen.produceDebugFiles = false

# Selects the simulation Strategy for TestGenAlgorithm
testgen.simulationStrategy = AUTOMATON_CONTROLLED
  enum:     [AUTOMATON_CONTROLLED, SAME_ALGORITHM_RESTART,
             SAME_ALGORITHM_FILTER_WAITLIST]

# algorithm stops on first found error path. Otherwise the algorithms tries
# to reach 100% coverage
testgen.stopOnError = false

# Output file Template under which the testcase automatons will be stored.
# Must include one %s somewhere.
testgen.testcaseOutputFile = "testcase%s.spc"

